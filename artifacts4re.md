An important aspect of engineering research is the study and production of research *artifacts*. An artifact is the tangible part of the research process, either produced in the domain of study, or produced by the researcher during the study. By tangible, we mean the artifact can be manipulated and exists independently of the study itself.

The open science era recognizes that artifacts are critical to the reproducibility of engineering research. It is difficult to validate claims about novel or important new findings without being able to analyze the underlying artifacts that were studied or created. This is why the notion of Artifact Evaluation Committees (AEC) has grown in importance. An AEC assesses the artifacts associated with a study (typically as part of a conference submission). As part of this assessment, AECs can assign a badge or marker indicating how suitable the artifacts are for reproducing the results of the study, ranging from the fact that artifacts exist (functional), all the way to artifacts that have been used in new studies (reproduced).

For the first time, the Requirements Engineering conference in 2019 will include artifact evaluation for those papers who wish to participate. A small AEC will evaluate accepted papers that submit for badging, and assign one or more of the five ACM/IEEE artifact badges.

# What is an artifact?
This naturally raises the question of what an RE artifact is. The wider software engineering community has created a [list of artifacts](https://github.com/researchart/all/blob/master/ListOfArtifacts.md) for AECs at ICSE18, FSE18, and ICSE19. The list, ranked in order of size, includes:
* research hypotheses
* statistical tests
* baseline results
* the actual paper itself
* data generated in the research, raw or derived
* executable models, and 
* delivery tools, such as containers and virtual machines.

# What is an artifact in RE? 
* Software, which are implementations of systems or algorithms potentially useful in other studies. For instance, tools to search for optimally consistent requirements specifications.
* Machine readable requirements models, e.g. in IstarML or UML interchange formats.
* Traceability relations between artifacts, such as requirements to source code.
* Data repositories, which are data (e.g., requirements models, requirements text, survey raw data) that can be used for multiple software engineering approaches.
* Requirements in natural language form, e.g. textual requirements in a spreadsheet, DOORS, JIRA export.
* User reviews, such as app reviews, product changelogs, and release notes.
* Frameworks, which are tools and services illustrating new approaches to requirements engineering that could be used by other researchers in different contexts. For example, a service to highlight inconsistencies in natural language.

# References

[Mendez et al software  artifact meta model](https://arxiv.org/abs/1806.00098)
