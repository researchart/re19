{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"04_ling_stats_calculator.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"clGGVB5R7FUD","colab_type":"text"},"cell_type":"markdown","source":["# Linguistic Features Stats Calculator\n","This file calculates stats for different linguistic features in the requirements in a set of datasets given as input\n","\n","Section 1. Imports and util functions\n","\n","Section 2. Stats calculation"]},{"metadata":{"id":"E0zkrIku7FUI","colab_type":"text"},"cell_type":"markdown","source":["## 1. Imports and Util functions"]},{"metadata":{"scrolled":true,"id":"P1W_qNXI7FUL","colab_type":"code","colab":{}},"cell_type":"code","source":["import spacy\n","from collections import defaultdict\n","from collections import OrderedDict\n","import pandas as pd\n","from tqdm import tqdm #progress bar\n","import numpy as np\n","import itertools\n","from nltk import Tree\n","from copy import copy, deepcopy\n","from spacy.lemmatizer import Lemmatizer\n","from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES\n","lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)\n","\n","# load the english language for spacy\n","nlp = spacy.load('en')\n","\n","# constants used to determine the type of linguistic feature to analyze\n","DEP_TYPE = 'Dependencies'\n","BR_TYPE = 'Branches'\n","SEQ_TYPE = 'Sequences'\n","VERB_TYPE = 'Verbs'\n","\n","\n","def count_req_types_frequencies(data):\n","    \"\"\"\n","    Counts\n","    num_tot: the nr. of req in the dataset (the size of the dataset) data\n","    num_F: the nr. of req annotated as functional in the dataset\n","    num_Q: the nr. of req annotated as quality\n","    num_FandQ: the nr. of req annotated as both functional and quality\n","    num_FnotQ: the nr. of req annotated as functional but not quality\n","    num_QnotF: the nr. of req annotated as quality but not functional\n","    num_I: the nr. of req annotated neither as quality nor as functional\n","    @param data: the dataset (a DataFrame)\n","    @return: a tuple with the values above described\n","    \"\"\"\n","    num_tot = 0.\n","    num_F = 0.\n","    num_Q = 0.\n","    num_FandQ = 0.\n","    num_FnotQ = 0.\n","    num_QnotF = 0.\n","    num_I = 0.\n","    \n","    idx = 0\n","    for x in data['RequirementText']:\n","        num_tot +=1\n","        if data.at[idx, 'IsFunctional'] == 1:\n","            num_F +=1\n","            if data.at[idx, 'IsQuality'] == 1:\n","                num_FandQ +=1\n","                num_Q +=1\n","            else:\n","                num_FnotQ +=1\n","        else:\n","            if data.at[idx, 'IsQuality'] == 1:\n","                num_QnotF +=1\n","                num_Q +=1\n","            else:\n","                num_I +=1\n","\n","        idx += 1\n","    \n","    return num_tot, num_F, num_Q, num_FandQ, num_FnotQ, num_QnotF, num_I\n","     \n","        \n","        \n","def pprint_dep_stats(header_length, d):\n","    \"\"\"\n","    Sorts the features by descending abs(cov_FnotQ-cov_QnotF), \n","    with cov_FnotQ and cov_QnotF respectively the % of req F but not Q and % of req Q but not F with the i-feature\n","    Then prints in a readable way the stats for the first header_length most significant (w.r.t. the sorting) features\n","    \"\"\"\n","    \n","    print(\"{:<12} {:<12} {:<12} {:<12} {:<12} {:<12} {:<12} {:<12}\"\n","      .format('Feat', 'Tot (cov)', 'Tot F (cov)', 'Tot Q (cov)', \n","              'FandQ (cov)', 'FnotQ (cov)', 'QnotF (cov)', 'I (cov)'))\n","\n","    \n","    #sorting the array by abs(cov_FnotQ-cov_QnotF)\n","    m = OrderedDict(sorted(d.items(), key=lambda x: abs(x[1][11]-x[1][9])))\n","    #alternative sorting by cov_FnotQ/cov_QnotF\n","    #m = OrderedDict(sorted(d.items(), key=lambda x: x[1][9]/x[1][11] if x[1][11]>0 else x[1][9]))\n","    #alternative sorting by cov_F/cov_QnotF\n","    #m = OrderedDict(sorted(d.items(), key=lambda x: x[1][3]/x[1][11] if x[1][11]>0 else x[1][3]))\n","    \n","    idx = 0\n","    for k, i in reversed(m.items()): #take the features in reversed order (descending order)\n","      if idx < min(header_length, len(m)):\n","        if i[9]>0.1 or i[11]>0.1:\n","          tot = str(i[0])+' ('+str(round(i[1], 1))+')'\n","          F = str(i[2])+' ('+str(round(i[3], 1))+')'\n","          Q = str(i[4])+' ('+str(round(i[5], 1))+')'\n","          FandQ = str(i[6])+' ('+str(round(i[7], 1))+')'\n","          FnotQ = str(i[8])+' ('+str(round(i[9], 1))+')'\n","          QnotF = str(i[10])+' ('+str(round(i[11], 1))+')'\n","          I = str(i[12])+' ('+str(round(i[13], 1))+')'\n","\n","          print(\"{:<12} {:<12} {:<12} {:<12} {:<12} {:<12} {:<12} {:<12}\"\n","                    .format(k, tot, F, Q, FandQ, FnotQ, QnotF, I)) \n","          idx+=1\n","      else:\n","        break\n","\n","        \n","        \n","def calc_stats(data, stats_type, n, seq_length=1, max_height=15):\n","    \"\"\"\n","    Calculates stats about dependencies in the requirements in dataset data\n","    @param data: the dataset (a DataFrame)\n","    @param stats_type: a value DEP_TYPE, BR_TYPE or SEQ_TYPE that determines the type of stats to calculate, \n","    (respectively the dependencies in the requirement, the branches of dependencies and the sequences of POSdep)\n","    if none of the three before mentioned types is expressed, stats for ROOT VERBS are calculated\n","    @param n: the combinations of features to consider \n","    (e.g., n=2 with stats_type=DEP_TYPE means that the stats will concern combinations of 2 dependencies in the req)\n","    @param seq_length: (optional) the maximum length of sequences of POSdep considered in case of stats_type=SEQ_TYPE\n","    @param max_height: (optional) the maximum height considered for the branches in case of stats_type=BR_TYPE\n","    @return: a tuple of dictionaries, each containing stats per feature for each type of req.\n","    Each element e in the tuple (e.g., e=dep_F) is a dictionary with key f (a feature) and two values: \n","    the number of features f found in data for req type e and the percentage of requirement of type e containing at least once f \n","    \"\"\"\n","    \n","    # the following dictionaries will be populated with the stats\n","    dep = defaultdict(lambda: [0,0])\n","    dep_F = defaultdict(lambda: [0,0])\n","    dep_Q = defaultdict(lambda: [0,0])\n","    dep_FandQ = defaultdict(lambda: [0,0])\n","    dep_FnotQ = defaultdict(lambda: [0,0])\n","    dep_QnotF = defaultdict(lambda: [0,0])\n","    dep_I = defaultdict(lambda: [0,0])\n","\n","    idx = 0\n","    for req in tqdm(data['RequirementText'], desc= str(n)+' '+str(stats_type)+' analysis', position=0):\n","#       print(req)\n","        doc = nlp(req.replace(\"'\", \"\")) #use spacy to annotate the requirement\n","#       print(\"Dependencies\", [(t.text, t.dep_, t.head.text) for t in doc])\n","\n","        req_counted = defaultdict(lambda: False)\n","    \n","        # 1. Create the list of features in the requirement, depending on the type stats_type\n","      \n","        #DEPENDENCY TYPES\n","        if stats_type == DEP_TYPE: \n","            req_dep = []\n","            for t in doc:\n","                req_dep.append(t.dep_)\n","            dep_comb = list(itertools.combinations(req_dep, n))\n","            dep_comb.sort()\n","\n","        #TYPES OF BRANCHES\n","        elif stats_type == BR_TYPE: \n","            dep_br_lists = [get_all_paths(sent.root, 0, max_height) for sent in doc.sents]\n","            dep_br = []\n","            for l in dep_br_lists:\n","                if l!=['ROOT']:\n","                    dep_br = dep_br + l\n","            dep_br.sort()\n","            dep_comb = list(itertools.combinations(dep_br, n))\n","            \n","        #SEQUENCES OF POSdep\n","        elif stats_type == SEQ_TYPE: \n","            req_dep = []\n","            for t in doc:\n","                req_dep.append(t.tag_+t.dep_)\n","            req_seq = []\n","            step = 1\n","            if n>1:\n","              step = seq_length\n","            for i in range(0, len(req_dep)-seq_length, step):\n","              s = ''\n","              for j in req_dep[i:i+seq_length]:\n","                s=s+(\"_\" if s!='' else '')+j\n","              req_seq.append([s])\n","\n","            req_seq.sort()\n","            dep_comb = list(itertools.combinations(req_seq, n))\n","            \n","        #ROOT VERBS\n","        else:         \n","          newr = req.replace('be able to', '').replace('be capable of', '').replace('provide the ability to', '')\n","          doc = nlp(newr)\n","          roots = []\n","          for t in doc:\n","            if t.dep_=='ROOT':\n","              roots.append(lemmatizer(t.orth_, t.pos_)[0])\n","          dep_comb = list(itertools.combinations(roots, n))\n","          dep_comb.sort()\n","          \n","        # 2. For each of the features created in step 1, if combinations of them are required, combine them\n","        for c in dep_comb:\n","            t = str(c[0])\n","            if(len(c)>1):\n","                for i in range(1,len(c)):\n","                    t = t+'+'+str(c[i])\n","            dep[t][0]+=1\n","            if not req_counted[t]:\n","                dep[t][1]+=1\n","                req_counted[t] = True\n","\n","        # 3. Update the stats of the correct type of the requirement, for the features (or their combinations) of step 2\n","        F_counted = defaultdict(lambda: False)\n","        Q_counted = defaultdict(lambda: False)\n","        FnQ_counted = defaultdict(lambda: False)\n","        QnF_counted = defaultdict(lambda: False)\n","        FaQ_counted = defaultdict(lambda: False)\n","        I_counted = defaultdict(lambda: False)\n","        if data.at[idx, 'IsFunctional'] == 1: #F\n","            for c in dep_comb:\n","                t = str(c[0])\n","                if(len(c)>1):\n","                    for i in range(1,len(c)):\n","                        t = t+'+'+str(c[i])\n","                dep_F[t][0]+=1\n","                if not F_counted[t]:\n","                    dep_F[t][1]+=1\n","                    F_counted[t] = True\n","\n","                if data.at[idx, 'IsQuality'] == 0: #OnlyF\n","                    dep_FnotQ[t][0]+=1\n","                    if not FnQ_counted[t]:\n","                        dep_FnotQ[t][1]+=1\n","                        FnQ_counted[t] = True\n","                else: #Q and FandQ\n","                    dep_FandQ[t][0]+=1\n","                    dep_Q[t][0]+=1\n","                    if not FaQ_counted[t]:\n","                        dep_FandQ[t][1]+=1\n","                        FaQ_counted[t] = True\n","                    if not Q_counted[t]:\n","                        dep_Q[t][1]+=1\n","                        Q_counted[t] = True\n","        else:\n","            for c in dep_comb:\n","                t = str(c[0])\n","                if(len(c)>1):\n","                    for i in range(1,len(c)):\n","                        t = t+'+'+str(c[i])\n","                if data.at[idx, 'IsQuality'] == 1: #OnlyQ\n","                    dep_QnotF[t][0]+=1\n","                    dep_Q[t][0]+=1\n","                    if not QnF_counted[t]:\n","                        dep_QnotF[t][1]+=1\n","                        QnF_counted[t] = True\n","                    if not Q_counted[t]:\n","                        dep_Q[t][1]+=1\n","                        Q_counted[t] = True\n","                else: #notR\n","                    dep_I[t][0]+=1\n","                    if not I_counted[t]: \n","                        dep_I[t][1]+=1\n","                        I_counted[t] = True\n","\n","        idx = idx + 1\n","    return dep, dep_F, dep_Q, dep_FandQ, dep_FnotQ, dep_QnotF, dep_I\n","          \n","    \n","def get_all_paths(node, h, max_h):\n","    \"\"\"\n","    Calculates all the dependencies paths (branches) in a requirement dependency tree up to an height of max_h\n","    @param node: the root of the tree\n","    @param h: the initial height (typically 0)\n","    @return: a list of strings representing paths\n","    \"\"\"\n","    if node.n_lefts + node.n_rights == 0 or h==max_h:\n","        return [node.dep_]\n","    return [\n","        node.dep_ + '_' + str(path) for child in node.children for path in get_all_paths(child, h+1, max_h)\n","    ]\n","  \n","  \n","\n","def createDict(dep_tot, dep_F, dep_Q, dep_FandQ, dep_FnotQ, dep_QnotF, dep_I, \n","               num_tot, num_F, num_Q, num_FandQ, num_FnotQ, num_QnotF, num_I):\n","  \"\"\"\n","  Creates a dictionary containing all the stats used in the analysis\n","  @params: t dictionaries, one for each type of requirements, obtained from function calc_stats \n","  and 7 integers, obtained from function count_req_types_frequencies\n","  @return: a dictionary\n","  \"\"\"\n","  d = {}\n","  for k, v in dep_tot.items():\n","    d[k] = [v[0], v[1]/num_tot if num_tot>0 else 0,\n","            dep_F[k][0], dep_F[k][1]/num_F if num_F>0 else 0,\n","            dep_Q[k][0], dep_Q[k][1]/num_Q if num_Q>0 else 0,\n","            dep_FandQ[k][0], dep_FandQ[k][1]/num_FandQ if num_FandQ>0 else 0,\n","            dep_FnotQ[k][0], dep_FnotQ[k][1]/num_FnotQ if num_FnotQ>0 else 0,\n","            dep_QnotF[k][0], dep_QnotF[k][1]/num_QnotF if num_QnotF>0 else 0,\n","            dep_I[k][0], dep_I[k][1]/num_I if num_I>0 else 0]\n","  return d\n","\n","\n","def addDicts(x, y):\n","  \"\"\"\n","  Add the values of two isomophic dictionaries\n","  \"\"\"\n","  result = dict(x)\n","  for k, v in y.items():\n","    if k in result:\n","      for i in range(0, len(v)): \n","        result[k][i] += v[i]\n","    else:\n","      result[k] = v\n","  return result"],"execution_count":0,"outputs":[]},{"metadata":{"id":"3Qi93XWu7FUX","colab_type":"text"},"cell_type":"markdown","source":["## 2. Stats Calculation"]},{"metadata":{"scrolled":false,"id":"V7Nti3gc7FUb","colab_type":"code","outputId":"f1b9dd3e-385e-4550-b7f7-9ead3af3c5cd","executionInfo":{"status":"ok","timestamp":1554640642551,"user_tz":-120,"elapsed":30262,"user":{"displayName":"Davide Dell'Anna","photoUrl":"https://lh5.googleusercontent.com/-dJU__MJ08Ww/AAAAAAAAAAI/AAAAAAAAbsA/YkubKaeFgQo/s64/photo.jpg","userId":"01259263475584376287"}},"colab":{"base_uri":"https://localhost:8080/","height":586}},"cell_type":"code","source":["data_folder = './' #can be an url\n","dataset_names = ['promise-reclass', 'ds2', 'ds3', 'dronology', 'wasp', 'esa-eucl-est', 'leeds', 'reqview']\n","\n","# read the datasets\n","datasets = [pd.read_csv(data_folder+dataset_name+'.csv', engine='python')[['RequirementText', 'IsFunctional', 'IsQuality']] for dataset_name in dataset_names] \n","\n","# # generates a new dataset combining all the others\n","# datasets_combined = pd.concat(datasets)\n","# datasets_combined = datasets_combined.reset_index(drop=True)\n","# # datasets_combined.to_csv(r'4ds_combined.csv')\n","# datasets = [datasets_combined]\n","\n","\n","#DEFINING THE EXPERIMENTS\n","stats_types = [DEP_TYPE, BR_TYPE, SEQ_TYPE, VERB_TYPE]\n","combinations = [1, 2, 3]\n","seq_lengths = [1,2,3,4]\n","VERBOSE = False #if True prints also the stats for the single datasets instead of only the final macro average\n","top_n_to_print = 10\n","\n","for stat_type in stats_types:\n","  for combination in combinations:\n","    for seq_l in seq_lengths:\n","      print(\"\\n== Experiment: \"+stat_type+\", \"+str(combination)+\" combinations, sequences long (if applies) \"+str(seq_l)+\" ==\")\n","      d = {} #matrix for the macro average\n","      d.clear()\n","      idx = 0\n","      for data in datasets:\n","          print('\\nDataset '+dataset_names[idx])\n","          idx+=1\n","          num_tot, num_F, num_Q, num_FandQ, num_FnotQ, num_QnotF, num_I = count_req_types_frequencies(data)\n","          dep_tot, dep_F, dep_Q, dep_FandQ, dep_FnotQ, dep_QnotF, dep_I = calc_stats(data, stat_type, combination, seq_l)\n","          data_d = createDict(dep_tot, dep_F, dep_Q, dep_FandQ, dep_FnotQ, dep_QnotF, dep_I, num_tot, num_F, num_Q, num_FandQ, num_FnotQ, num_QnotF, num_I)\n","          if VERBOSE:\n","            pprint_dep_stats(top_n_to_print, data_d)\n","          if not d:\n","            d = dict(data_d)\n","          else: \n","            d = addDicts(d, data_d)\n","      for k, v in d.items(): #calc the macro average\n","        d[k] = [((v[j] / len(datasets)) if j%2!=0 else  v[j]) for j in range(0, len(v))]\n","\n","      print(\"\")\n","      print(\"All together with cov averaged over the datasets\")\n","      pprint_dep_stats(top_n_to_print, d)\n","      \n","      if not stat_type=='Sequences': #do not continue the inner loop if the type does not contain sequences (avoid repeating twice same things)\n","        break"],"execution_count":0,"outputs":[{"output_type":"stream","text":["1 Dependencies analysis:   0%|          | 7/1502 [00:00<00:24, 60.60it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","== Experiment: Dependencies, 1 combinations, sequences long (if applies) 1 ==\n","\n","Dataset 8combined\n"],"name":"stdout"},{"output_type":"stream","text":["1 Dependencies analysis: 100%|██████████| 1502/1502 [00:29<00:00, 53.32it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","All together with cov averaged over the datasets\n","Feat         Tot (cov)    Tot F (cov)  Tot Q (cov)  FandQ (cov)  FnotQ (cov)  QnotF (cov)  I (cov)     \n","acl          466 (0.3)    365 (0.3)    169 (0.2)    72 (0.2)     293 (0.3)    97 (0.2)     4 (0.2)     \n","ccomp        195 (0.1)    140 (0.1)    97 (0.1)     42 (0.1)     98 (0.1)     55 (0.1)     0 (0.0)     \n","dobj         1862 (0.8)   1373 (0.9)   848 (0.7)    378 (0.9)    995 (0.9)    470 (0.6)    19 (0.8)    \n","advcl        325 (0.2)    235 (0.2)    156 (0.2)    68 (0.2)     167 (0.2)    88 (0.2)     2 (0.1)     \n","mark         199 (0.1)    140 (0.1)    90 (0.1)     32 (0.1)     108 (0.2)    58 (0.1)     1 (0.1)     \n","xcomp        434 (0.3)    282 (0.3)    235 (0.3)    84 (0.3)     198 (0.3)    151 (0.2)    1 (0.1)     \n","nsubj        1842 (0.9)   1187 (0.9)   973 (0.9)    338 (0.9)    849 (0.9)    635 (0.9)    20 (0.9)    \n","det          4236 (1.0)   2779 (1.0)   2156 (1.0)   737 (1.0)    2042 (1.0)   1419 (1.0)   38 (1.0)    \n","ROOT         1752 (1.0)   1043 (1.0)   1007 (1.0)   318 (1.0)    725 (1.0)    689 (1.0)    20 (1.0)    \n","aux          2589 (1.0)   1678 (1.0)   1385 (1.0)   495 (1.0)    1183 (1.0)   890 (1.0)    21 (0.9)    \n","compound     2685 (0.7)   1623 (0.7)   1508 (0.8)   492 (0.8)    1131 (0.7)   1016 (0.8)   46 (0.8)    \n","cc           904 (0.4)    557 (0.4)    512 (0.5)    183 (0.5)    374 (0.4)    329 (0.4)    18 (0.8)    \n","conj         1121 (0.5)   724 (0.4)    613 (0.5)    239 (0.5)    485 (0.4)    374 (0.5)    23 (0.9)    \n","pobj         3150 (0.9)   1840 (0.9)   1912 (0.9)   623 (0.9)    1217 (0.9)   1289 (0.9)   21 (0.6)    \n","prep         3055 (0.9)   1774 (0.9)   1848 (0.9)   590 (0.9)    1184 (0.8)   1258 (0.9)   23 (0.6)    \n","punct        2587 (0.9)   1543 (0.9)   1514 (0.9)   507 (0.9)    1036 (0.9)   1007 (0.9)   37 (0.7)    \n","amod         1650 (0.6)   939 (0.6)    1023 (0.7)   335 (0.7)    604 (0.6)    688 (0.7)    23 (0.6)    \n","advmod       526 (0.3)    289 (0.3)    363 (0.3)    131 (0.4)    158 (0.2)    232 (0.3)    5 (0.2)     \n","acomp        471 (0.3)    264 (0.3)    283 (0.3)    77 (0.3)     187 (0.3)    206 (0.3)    1 (0.1)     \n","nsubjpass    400 (0.2)    223 (0.2)    265 (0.3)    94 (0.3)     129 (0.2)    171 (0.3)    6 (0.3)     \n","auxpass      445 (0.2)    240 (0.2)    294 (0.3)    96 (0.3)     144 (0.2)    198 (0.3)    7 (0.3)     \n","nmod         254 (0.1)    137 (0.1)    182 (0.2)    67 (0.2)     70 (0.1)     115 (0.2)    2 (0.1)     \n","nummod       379 (0.2)    94 (0.1)     333 (0.3)    52 (0.2)     42 (0.1)     281 (0.3)    4 (0.2)     \n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]}]}