{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01_KM_reconstruction.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1pBEJLFXgGd",
        "colab_type": "text"
      },
      "source": [
        "# RE19-classification: reconstruction of Kurtanovic-Maalej\n",
        "\n",
        "This notebook takes as input the technique presented by Kurtanovic and Maalej at RE'17 (data track), and reconstructs it on the Promise dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4UYLLiWXgGp",
        "colab_type": "text"
      },
      "source": [
        "## 0. Set up (optional)\n",
        "\n",
        "Run the following  install functions if running Jupyter on a cloud environment like Colaboratory, which does not allow you to install the libraries permanently on your local machine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMoT1F60XgGy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install cython numpy\n",
        "!pip install benepar[cpu]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPplS2cpXgHN",
        "colab_type": "text"
      },
      "source": [
        "## 1. Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4nPAVAEXgHY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Basic numpy, sklearn, pandas libraries\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from IPython.display import display\n",
        "\n",
        "# Basic NLTK tooling\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "\n",
        "# The benepar parser -- this is supposed to be a better parser than Stanford's parser used in the RE'17 paper\n",
        "import benepar\n",
        "benepar.download('benepar_en2')\n",
        "\n",
        "# Tqdm, for progress bars -- useful to show that the parsing is working\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5O8ArruqXgHv",
        "colab_type": "text"
      },
      "source": [
        "## 2. Load data\n",
        "\n",
        "Imports the classified data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7jMWl49XgHy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading the re-classified data set PROMISE\n",
        "DATA_FOLDER =  './'\n",
        "data = pd.read_csv(DATA_FOLDER+'promise-reclass.csv', engine='python')\n",
        "\n",
        "print (data.head())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBTYma3AXgIE",
        "colab_type": "text"
      },
      "source": [
        "## 3. Dataset enrichment\n",
        "\n",
        "Additional features are added automatically, as per the RE'17 paper."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLmhUSlmXgIJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Text length\n",
        "data['Length'] = 0\n",
        "idx = 0\n",
        "for x in data['RequirementText']:\n",
        "    data.at[idx, 'Length'] = len(x)\n",
        "    idx = idx + 1  \n",
        "\n",
        "# POS tags and tree information\n",
        "parser = benepar.Parser(\"benepar_en2\")\n",
        "data['Modal'] = 0.0\n",
        "data['Adjective'] = 0.0\n",
        "data['Noun'] = 0.0\n",
        "data['Adverb'] = 0.0\n",
        "data['Verb'] = 0.0\n",
        "data['TreeHeight'] = 0\n",
        "data['SubTrees'] = 0\n",
        "idx = 0\n",
        "for req in tqdm(data['RequirementText'], desc='Parse trees', position=0):\n",
        "    tokens = tokenizer.tokenize(req)\n",
        "    data.at[idx, 'Words'] = len(tokens)\n",
        "    tags = nltk.pos_tag(tokens)\n",
        "    fd = nltk.FreqDist(tag for (word, tag) in tags)\n",
        "    for key, value in fd.items():\n",
        "        if key==\"MD\":\n",
        "            data.at[idx, 'Modal'] = value\n",
        "        if key.startswith(\"JJ\"):\n",
        "            data.at[idx, 'Adjective'] = value\n",
        "        if key.startswith(\"VB\"):\n",
        "            data.at[idx, 'Verb'] = value\n",
        "        if key.startswith(\"NN\"):\n",
        "            data.at[idx, 'Noun'] = value\n",
        "        if key==\"RB\":\n",
        "            data.at[idx, 'Adverb'] = value\n",
        "    data.at[idx, 'Modal'] = data.at[idx, 'Modal'] / len(tokens)\n",
        "    data.at[idx, 'Adjective'] = data.at[idx, 'Adjective'] / len(tokens)\n",
        "    data.at[idx, 'Noun'] = data.at[idx, 'Noun'] / len(tokens)\n",
        "    data.at[idx, 'Adverb'] = data.at[idx, 'Adverb'] / len(tokens)\n",
        "    data.at[idx, 'Verb'] = data.at[idx, 'Verb'] / len(tokens)       \n",
        "    tree = parser.parse(req)\n",
        "    data.at[idx, 'TreeHeight'] = tree.height()\n",
        "    data.at[idx, 'SubTrees'] = len(tree)\n",
        "    idx = idx + 1    \n",
        "    \n",
        "print(data[:30])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9cYs59nXgIe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.util import ngrams\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction import stop_words\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "bigrams = []\n",
        "trigrams = []\n",
        "frequencies = Counter([])\n",
        "frequencies2 = Counter([])\n",
        "frequencies3 = Counter([])\n",
        "pfrequencies = Counter([])\n",
        "pfrequencies2 = Counter([])\n",
        "pfrequencies3 = Counter([])\n",
        "\n",
        "wn_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Generation of [1, 2, 3] textgrams, [1, 2, 3] POSgrams\n",
        "# Fix with: tokenize, remove stopwords, lemmatize, then \n",
        "for req in tqdm(data['RequirementText'], desc='n-grams generation', position=0):\n",
        "    token = tokenizer.tokenize(req)\n",
        "    token = [word.lower() for word in token]\n",
        "    tags = nltk.pos_tag(token)\n",
        "    token = [w for w in token if not w in stop_words.ENGLISH_STOP_WORDS]\n",
        "    token = [wn_lemmatizer.lemmatize(w) for w in token]\n",
        "    frequencies += Counter(token)\n",
        "    bigrams = ngrams(token,2)\n",
        "    trigrams = ngrams(token,3)\n",
        "    frequencies2 += Counter(bigrams)\n",
        "    frequencies3 += Counter(trigrams)\n",
        "    punigrams = [tag for (word, tag) in tags]\n",
        "    pfrequencies += Counter(punigrams)\n",
        "    pbigrams = ngrams([tag for (word, tag) in tags], 2)\n",
        "    pfrequencies2 += Counter(pbigrams)\n",
        "    ptrigrams = ngrams([tag for (word, tag) in tags], 3)\n",
        "    pfrequencies3 += Counter(ptrigrams)\n",
        "\n",
        "# Labeling of the features\n",
        "for f in list(frequencies):\n",
        "  label = '_' + f + '_'\n",
        "  data[label] = 0\n",
        "\n",
        "for f in list(frequencies2):\n",
        "  label = '_' + f[0] + '_' + f[1] + '_'\n",
        "  data[label] = 0\n",
        "\n",
        "for f in list(frequencies3):\n",
        "  label = '_' + f[0] + '_' + f[1] + '_' + f[2] + '_'\n",
        "  data[label] = 0\n",
        "\n",
        "for f in list(pfrequencies):\n",
        "  label = f\n",
        "  data[label] = 0\n",
        "  \n",
        "for f in list(pfrequencies2):\n",
        "  label = f[0] + '_' + f[1]\n",
        "  data[label] = 0\n",
        "\n",
        "for f in list(pfrequencies3):\n",
        "  label = f[0] + '_' + f[1] + '_' + f[2]\n",
        "  data[label] = 0\n",
        "  \n",
        "print (len(frequencies), len(frequencies2), len(frequencies3), len(pfrequencies), len(pfrequencies2), len(pfrequencies3))\n",
        "\n",
        "# Populating the n-grams\n",
        "idx = 0\n",
        "for req in tqdm(data['RequirementText'], desc='n-grams population', position=0):\n",
        "    token = tokenizer.tokenize(req)\n",
        "\n",
        "    for t in token:\n",
        "      exists = [col for col in data.columns if col == str('_' + t + '_')]\n",
        "      if exists != []:\n",
        "        data.at[idx, exists] = 1\n",
        "      \n",
        "    bigrams = ngrams(token,2)\n",
        "    for bg in bigrams:\n",
        "      exists = [col for col in data.columns if col == str('_' + bg[0] + '_' + bg[1] + '_')]\n",
        "      if exists != []:\n",
        "        data.at[idx, exists] = 1\n",
        "    \n",
        "    trigrams = ngrams(token,3)\n",
        "    for tg in trigrams:\n",
        "      exists = [col for col in data.columns if col == str('_' + tg[0] + '_' + tg[1] + '_' + tg[2] + '_')]\n",
        "      if exists != []:\n",
        "        data.at[idx, exists] = 1\n",
        "    \n",
        "    tags = nltk.pos_tag(token)\n",
        "\n",
        "    for t in tags:\n",
        "      exists = [col for col in data.columns if col == str(t)]\n",
        "      if exists != []:\n",
        "        data.at[idx, exists] = 1\n",
        "        \n",
        "    pbigrams = ngrams([tag for (word, tag) in tags], 2)\n",
        "    for bg in pbigrams:\n",
        "      exists = [col for col in data.columns if col == str(bg[0] + '_' + bg[1])]\n",
        "      if exists != []:\n",
        "        data.at[idx, exists] = 1\n",
        "\n",
        "    ptrigrams = ngrams([tag for (word, tag) in tags], 3)\n",
        "    for tg in ptrigrams:\n",
        "      exists = [col for col in data.columns if col == str(tg[0] + '_' + tg[1] + '_' + tg[2])]\n",
        "      if exists != []:\n",
        "        data.at[idx, exists] = 1\n",
        "    \n",
        "    idx = idx + 1\n",
        "\n",
        "data.columns = data.columns.map(str)\n",
        "\n",
        "print (data.head())\n",
        "\n",
        "# The enriched dataset is now saved\n",
        "data.to_csv('dataset-full.csv', encoding='utf-8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRTVSSpsXgJA",
        "colab_type": "text"
      },
      "source": [
        "## 4. Feature reduction\n",
        "\n",
        "We reduce the dimensionality of the data. Change the *target*  parameter in the second cell to determine whether you want to train a classifier for F, Q, only F, or only Q requirements.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6RqNFqjXgJ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creation of an ensemble that uses adaptive boost, gradient boos, extra trees, and random forest\n",
        "def createTop (nfeatures, data, X_train, y_train, target):\n",
        "  #nfeatures = 100\n",
        "\n",
        "  ada_boost_clf = AdaBoostClassifier(random_state=42, n_estimators=30)\n",
        "  ada_boost_clf.fit(X_train, y_train)\n",
        "\n",
        "  gradient_boost_clf = GradientBoostingClassifier(random_state=42, n_estimators=30, max_depth = 5)\n",
        "  gradient_boost_clf.fit(X_train, y_train)\n",
        "\n",
        "  extra_trees_clf = ExtraTreesClassifier(random_state=42, n_estimators=30, max_depth = 5)\n",
        "  extra_trees_clf.fit(X_train, y_train)\n",
        "\n",
        "  random_forest_clf = RandomForestClassifier(random_state=42, n_estimators=30, max_depth = 5)\n",
        "  random_forest_clf.fit(X_train, y_train)\n",
        "\n",
        "  # Sorting in order of importance: average importance\n",
        "  importances = ada_boost_clf.feature_importances_  + gradient_boost_clf.feature_importances_ + extra_trees_clf.feature_importances_ + random_forest_clf.feature_importances_\n",
        "  indices = np.argsort(importances)[::-1]\n",
        "\n",
        "  # Print the feature ranking\n",
        "  print(\"Feature ranking:\")\n",
        "\n",
        "  tokeep = []\n",
        "  for f in range(0, nfeatures):\n",
        "      print(\"%d. feature %s (%f)\" % (f + 1, X_train.columns[indices[f]], importances[indices[f]]))\n",
        "      tokeep.append(X_train.columns[indices[f]])\n",
        "\n",
        "  tokeep.append('RequirementText')\n",
        "  tokeep.append('ProjectID')\n",
        "  tokeep.append('Class')\n",
        "  if target=='OnlyQuality':\n",
        "    tokeep.append('OnlyQuality')\n",
        "    tokeep.append('IsFunctional')\n",
        "    appendix = 'oq'\n",
        "  elif target=='OnlyFunctional':\n",
        "    tokeep.append('IsQuality')\n",
        "    tokeep.append('OnlyFunctional')\n",
        "    appendix = 'of'\n",
        "  elif target=='IsQuality' or target=='IsFunctional':\n",
        "    tokeep.append('IsQuality')\n",
        "    tokeep.append('IsFunctional')\n",
        "    if target=='IsQuality':\n",
        "      appendix = 'q'\n",
        "    else:\n",
        "      appendix = 'f'\n",
        "\n",
        "  data3 = data[tokeep]\n",
        "\n",
        "  print (data3.head())\n",
        "  data3.to_csv('promise-km-' + str(nfeatures) + '-' + appendix + '.csv', encoding='utf-8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRzoz1KjXgJJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set the target: choose between IsFunctional, IsQuality, OnlyFunctional, OnlyQuality\n",
        "target = 'OnlyQuality'\n",
        "\n",
        "data = pd.read_csv('dataset-full.csv', engine='python')\n",
        "datarep = data.drop(data.columns[0], axis=1)\n",
        "\n",
        "if target=='OnlyQuality':\n",
        "  datarep['OnlyQuality'] = ~datarep['IsFunctional'] & datarep['IsQuality']\n",
        "  todrop = ['RequirementText', 'Class', 'ProjectID', 'IsFunctional', 'IsQuality']\n",
        "\n",
        "if target=='OnlyFunctional':\n",
        "  datarep['OnlyFunctional'] = datarep['IsFunctional'] & ~datarep['IsQuality']\n",
        "  todrop = ['RequirementText', 'Class', 'ProjectID', 'IsFunctional', 'IsQuality']\n",
        "\n",
        "if target=='IsQuality':\n",
        "  todrop = ['RequirementText', 'Class', 'ProjectID', 'IsFunctional']\n",
        "\n",
        "if target=='IsFunctional':\n",
        "  todrop = ['RequirementText', 'Class', 'ProjectID', 'IsQuality']\n",
        "\n",
        "\n",
        "# Remove the features that are not used for the classification\n",
        "data2 = datarep.drop(todrop, axis = 1)\n",
        "\n",
        "# Create training and testing set\n",
        "# === BEGIN REMOVED AFTER CONDITIONAL ACCEPT\n",
        "# X_train, X_test, y_train, y_test = train_test_split(\n",
        "#      data2.drop([target], axis=1), data2[target], test_size=0.25, random_state=42)\n",
        "# print (X_train.columns)\n",
        "# === END REMOVED AFTER CONDITIONAL ACCEPT\n",
        "\n",
        "\n",
        "# === BEGIN REMOVED AFTER CONDITIONAL ACCEPT\n",
        "# createTop (500, datarep, X_train, y_train, target)\n",
        "# === END REMOVED AFTER CONDITIONAL ACCEPT\n",
        "\n",
        "# === BEGIN ADDED AFTER CONDITIONAL ACCEPT\n",
        "createTop (100, datarep, data2.drop([target], axis=1), data2[target], target)\n",
        "# === BEGIN REMOVED  AFTER CONDITIONAL ACCEPT\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rynuLsrWXgKN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}