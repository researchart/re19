{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03_KM_classif_performance.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nUFFixdJbqY",
        "colab_type": "text"
      },
      "source": [
        "# RE19-classification: performance evaluation of datasets enriched with KM features\n",
        "\n",
        "This notebook evaluates the performance of a F and Q requirements classifiers using KM features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "c4LfzHoKRUwk"
      },
      "source": [
        "## 0. Set up (optional)\n",
        "\n",
        "Run the following three  functions if running Jupyter on a cloud environment like Colaboratory, which does not allow you to install the libraries permanently on your local machine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "inoDnAn5savZ",
        "colab": {}
      },
      "source": [
        "#!git clone https://github.com/rulematrix/rule-matrix-py.git\n",
        "#!pip install rule-matrix-py/.\n",
        "#!pip install mdlp-discretization\n",
        "#!pip install pysbrl==0.4.2rc0\n",
        "#!pip install fim\n",
        "!pip install cython numpy\n",
        "#!pip install skope-rules\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "D9z9eI1OReXJ"
      },
      "source": [
        "## 1. Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5nFWJMhhsjed",
        "colab": {}
      },
      "source": [
        "#import rulematrix\n",
        "#from rulematrix.surrogate import rule_surrogate\n",
        "#from skrules import SkopeRules\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn import svm\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
        "from sklearn.metrics import recall_score, precision_score, f1_score, roc_curve, precision_recall_curve, auc, confusion_matrix, accuracy_score, roc_auc_score\n",
        "from imblearn.over_sampling import ADASYN \n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_breast_cancer, load_iris\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "from scipy import interp\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Set the ipython display in such a way that helps the visualization of the rulematrix outputs.\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "display(HTML(data=\"\"\"\n",
        "<style>\n",
        "    div#notebook-container    { width: 95%; }\n",
        "    div#menubar-container     { width: 65%; }\n",
        "    div#maintoolbar-container { width: 99%; }\n",
        "</style>\n",
        "\"\"\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7o09bmgJbq-",
        "colab_type": "text"
      },
      "source": [
        "## 2. Auxiliary functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WikMlK2sJbrB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def drop_descriptive_columns(dataset):\n",
        "    \"\"\"\n",
        "    Removes from a dataset, descriptive columns before using it for training the classifiers\n",
        "    @param dataset: the dataset enriched with features\n",
        "    @return: the new 'cleaned' dataset\n",
        "    \"\"\"\n",
        "    for c in dataset.columns:\n",
        "        if c in ['RequirementText', 'Class', 'ProjectID']:\n",
        "            dataset = dataset.drop(c, axis = 1)\n",
        "    return dataset\n",
        "\n",
        "def split_tr_te(dataset, target, to_drop, tsize=0.25):\n",
        "    \"\"\"\n",
        "    Splits a dataset in training and test set (1-tsize and tsize%)\n",
        "    @param dataset: the dataset to split\n",
        "    @param target: the target class\n",
        "    @param to_drop: some additional columns to drop before splitting\n",
        "    @param tsize: (optional) the size of the test set\n",
        "    @return: a tuple train_x, test_x, train_y, test_y, with y the target column, x the rest\n",
        "    \"\"\"\n",
        "    return train_test_split(dataset.drop(to_drop, axis=1), dataset[target], test_size=tsize, random_state=42)\n",
        "\n",
        "def print_scores(actual, pred, name, prob):\n",
        "    \"\"\"\n",
        "    Prints the confusion matrix given the results of a classifier and calculates precision, recall, f1 and AUC score\n",
        "    @param actual: the original annotation of the dataset (to use for the comparison in order to calculate the above metrics)\n",
        "    @param pred: the predictions made by the classifier\n",
        "    @param name: some textual variable to use for verbosity purposes\n",
        "    @param prob: vector with the probabilities for the predictions in pred\n",
        "    @return: a list [name, precision, recall, f1, auc]\n",
        "    \"\"\"\n",
        "    f1 = f1_score(actual, pred, average='micro') \n",
        "    prec = precision_score(actual, pred) \n",
        "    rec = recall_score(actual, pred) \n",
        "    auc = roc_auc_score(actual, prob)\n",
        "    print('=====', name)\n",
        "    print('Confusion matrix (test)\\n', confusion_matrix(actual, pred))\n",
        "#     print('F1-Score (micro)', f1)\n",
        "#     print('Precision', prec)\n",
        "#     print('Recall (train)', rec, '\\n')\n",
        "    return [name, prec, rec, f1, auc]\n",
        "    \n",
        "def build_plot(y_true=[], scores=[], labels=[]):\n",
        "    \"\"\"\n",
        "    Generates two plots: a roc plot and a preision/recall plot\n",
        "    \"\"\"\n",
        "    gradient = np.linspace(0, 1, 10)\n",
        "    color_list = [ cm.tab10(x) for x in gradient ]\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 5),\n",
        "                         sharex=True, sharey=True)\n",
        "    ax = axes[0]\n",
        "    n_line = 0\n",
        "    for i_score, score in enumerate(scores):\n",
        "        fpr, tpr, _ = roc_curve(y_true[n_line], score, drop_intermediate=False)\n",
        "        n_line = n_line + 1\n",
        "        ax.plot(fpr, tpr, linestyle='-.', c=color_list[i_score], lw=1, label=labels[i_score])\n",
        "    ax.set_title(\"ROC\", fontsize=20)\n",
        "    ax.set_xlabel('False Positive Rate', fontsize=18)\n",
        "    ax.set_ylabel('True Positive Rate (Recall)', fontsize=18)\n",
        "    ax.legend(loc='lower center', fontsize=8)\n",
        "\n",
        "    ax = axes[1]\n",
        "    n_line = 0\n",
        "    for i_score, score in enumerate(scores):\n",
        "        precision, recall, _ = precision_recall_curve(y_true[n_line], score)\n",
        "        n_line = n_line + 1\n",
        "        ax.step(recall, precision, linestyle='-.', c=color_list[i_score], lw=1, where='post', label=labels[i_score])\n",
        "    ax.set_title(\"Precision-Recall\", fontsize=20)\n",
        "    ax.set_xlabel('Recall (True Positive Rate)', fontsize=18)\n",
        "    ax.set_ylabel('Precision', fontsize=18)\n",
        "    ax.legend(loc='lower center', fontsize=8)\n",
        "    plt.show()\n",
        "\n",
        "        \n",
        "def train_classifier(model, train_x, train_y, name):\n",
        "    \"\"\"\n",
        "    Train a classifier and returns the fit scores for the training set\n",
        "    \"\"\"    \n",
        "    model.fit(train_x, train_y)\n",
        "    pred_train = model.predict(train_x)\n",
        "    prob = model.predict_proba(train_x)[:, 1]\n",
        "    scores_line = print_scores(train_y, pred_train, name, prob)\n",
        "    return scores_line, pred_train, prob\n",
        "  \n",
        "def evaluate_classifier(model, test_x, test_y, name):\n",
        "    \"\"\"\n",
        "    Executes the classifiers on the test set and returns the obtained scores\n",
        "    \"\"\"\n",
        "    pred_test = model.predict(test_x)\n",
        "    prob = model.predict_proba(test_x)[:, 1]\n",
        "    scores_line = print_scores(test_y, pred_test, name, prob)\n",
        "    return scores_line, pred_test, prob\n",
        "  \n",
        "def makeOverSamplesADASYN(X,y):\n",
        "  \"\"\"\n",
        "  Creates new data with oversampled variables by using ADASYN\n",
        "  @param X: Independent Variable in DataFrame\n",
        "  @param y: dependent variable in Pandas DataFrame formats\n",
        "  @return: an oversampled version of the variables\n",
        "  \"\"\"\n",
        "  sm = ADASYN()\n",
        "  X, y = sm.fit_sample(X, y)\n",
        "  return X, y\n",
        "\n",
        "def make_roc_curve(appendix, target, to_drop, golds, probs, names, scores, nrfeat, colors):\n",
        "  \"\"\"\n",
        "  Generates a ROC plot (used in the paper)\n",
        "  \"\"\"\n",
        "  cv = StratifiedKFold(n_splits=10)\n",
        "  classifier = svm.SVC(kernel='linear', probability=True, random_state=0)\n",
        "\n",
        "  # For fast processing\n",
        "  # from sklearn.ensemble import GradientBoostingClassifier\n",
        "  # classifier = GradientBoostingClassifier(random_state=42, n_estimators=30, max_depth = 5)\n",
        "\n",
        "  tprs = []\n",
        "  aucs = []\n",
        "  paucs = []\n",
        "  ptprs = []\n",
        "  mean_fpr = np.linspace(0, 1, 100)\n",
        "  pmean_fpr = np.linspace(0, 1, 100)\n",
        "\n",
        "  plt.figure(figsize=(10,6))\n",
        "\n",
        "  dataz = pd.read_csv(folder_datasets+'promise-km-' + str(nrfeat) + '-' + appendix + '.csv', engine='python')\n",
        "\n",
        "  # Attempt with project-based fold -- TODO: try another partitioning\n",
        "  # projects = [[3, 9, 11], [1, 5, 12], [6, 10, 13], [1, 8, 14], [3, 10, 12], [2, 5, 11], [4, 6, 14], [7, 8, 13], [2, 9, 15], [4, 7, 15] ]\n",
        "  projects = [[3, 9, 11], [1, 5, 12], [6, 10, 13], [1, 8, 14], [3, 12, 15], [2, 5, 11], [6, 9, 14], [7, 8, 13], [2, 4, 15], [4, 7, 10] ]\n",
        "  \n",
        "  print (target + 'p-fold')\n",
        "  prec = 0.0\n",
        "  rec = 0.0\n",
        "  f1 = 0.0\n",
        "  for k in projects:\n",
        "    mytest = dataz.loc[dataz['ProjectID'].isin(k)]\n",
        "    mytrain = dataz.loc[~dataz['ProjectID'].isin(k)]\n",
        "    mytest = drop_descriptive_columns(mytest)\n",
        "    mytest = mytest.drop(mytest.columns[0], axis=1)\n",
        "    mytrain = drop_descriptive_columns(mytrain)\n",
        "    mytrain = mytrain.drop(mytrain.columns[0], axis=1)\n",
        "    myprobs = classifier.fit(mytrain.drop(to_drop, axis=1), \n",
        "                             mytrain[target]).predict_proba(mytest.drop(to_drop, axis=1))\n",
        "    pred = classifier.predict(mytest.drop(to_drop, axis=1))\n",
        "    prec += precision_score(mytest[target].values.tolist(), pred) \n",
        "    rec += recall_score(mytest[target].values.tolist(), pred)\n",
        "    f1 += f1_score(mytest[target].values.tolist(), pred)\n",
        "    print (k, 'Precision', prec, 'Recall', rec )\n",
        "    myfpr, mytpr, _ = roc_curve(mytest[target].values.tolist(), myprobs[:, 1], drop_intermediate=False)\n",
        "    ptprs.append(interp(pmean_fpr, myfpr, mytpr))\n",
        "    ptprs[-1][0] = 0.0\n",
        "    my_auc = auc(myfpr, mytpr)\n",
        "#     my_auc = roc_auc_score(mytest[target].values.tolist(), myprobs[:, 1])\n",
        "    paucs.append(my_auc)\n",
        "    plt.plot(myfpr, mytpr, lw=1, color=colors['Promise test set'], alpha=0.8, linestyle='--',\n",
        "                 label='Projects bundle %s (AUC = %0.2f)' % (str(k), my_auc))\n",
        "\n",
        "  print ('p-fold', 'Precision', str(prec/10.0), 'Recall', str(rec/10.0), 'F1', str(f1/10.0), 'AUC', str(my_auc/10.0))  \n",
        "    \n",
        "  \n",
        "  pmean_tpr = np.mean(ptprs, axis=0)\n",
        "  pmean_tpr[-1] = 1.0\n",
        "#   pmean_auc = auc(pmean_fpr, pmean_tpr)\n",
        "  pmean_auc = np.mean(paucs, axis=0)\n",
        "  std_auc = np.std(paucs)\n",
        "  plt.plot(pmean_fpr, pmean_tpr, color=colors['Promise test set'], linestyle='--',\n",
        "           label=r'Mean p-fold (AUC = %0.2f $\\pm$ %0.2f)' % (pmean_auc, std_auc),\n",
        "           lw=2, alpha=.8)\n",
        "\n",
        "  std_tpr = np.std(ptprs, axis=0)\n",
        "  tprs_upper = np.minimum(pmean_tpr + std_tpr, 1)\n",
        "  tprs_lower = np.maximum(pmean_tpr - std_tpr, 0)\n",
        "  plt.fill_between(pmean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
        "                   label=r'$\\pm$ 1 std. dev. from p-fold')\n",
        "  \n",
        "  plt.xlim([-0.01, 1.01])\n",
        "  plt.ylim([-0.01, 1.01])\n",
        "  plt.xlabel('False Positive Rate')\n",
        "  plt.ylabel('True Positive Rate')\n",
        "  #plt.title('Receiver operating characteristic')\n",
        "  plt.legend(loc=\"lower right\")\n",
        "  plt.show()\n",
        "\n",
        "  plt.figure(figsize=(10,6))\n",
        "  \n",
        "  dataz = drop_descriptive_columns(dataz)\n",
        "  dataz = dataz.drop(dataz.columns[0], axis=1)\n",
        "\n",
        "  X = dataz.drop(to_drop, axis=1)\n",
        "  y = dataz[target]\n",
        "\n",
        "  # This code plots the ROC curve with cross validation\n",
        "  print (target + 'k-fold')\n",
        "  i = 0\n",
        "  prec = 0.0\n",
        "  rec = 0.0\n",
        "  f1 = 0.0\n",
        "  for train, test in cv.split(X, y):\n",
        "      probas_ = classifier.fit(X.iloc[train], y.iloc[train]).predict_proba(X.iloc[test])\n",
        "      pred = classifier.predict(X.iloc[test])\n",
        "      prec += precision_score(y.iloc[test], pred) \n",
        "      rec += recall_score(y.iloc[test], pred)\n",
        "      f1 += f1_score(y.iloc[test], pred)\n",
        "      print (i, 'Precision', prec, 'Recall', rec )\n",
        "      # Compute ROC curve and area the curve\n",
        "      fpr, tpr, thresholds = roc_curve(y.iloc[test], probas_[:, 1], drop_intermediate=False)\n",
        "      tprs.append(interp(mean_fpr, fpr, tpr))\n",
        "      tprs[-1][0] = 0.0\n",
        "      roc_auc = auc(fpr, tpr)\n",
        "#       roc_auc = roc_auc_score(y.iloc[test], probas_[:, 1])\n",
        "      aucs.append(roc_auc)\n",
        "      plt.plot(fpr, tpr, lw=1, alpha=0.3,\n",
        "               label='k-fold %d (AUC = %0.2f)' % (i, roc_auc))\n",
        "      i += 1\n",
        "  \n",
        "  print ('k-fold', 'Precision', str(prec/10.0), 'Recall', str(rec/10.0), 'F1', str(f1/10.0), 'AUC', str(roc_auc/10))\n",
        "  \n",
        "  \n",
        "  plt.xlim([-0.01, 1.01])\n",
        "  plt.ylim([-0.01, 1.01])\n",
        "  plt.xlabel('False Positive Rate')\n",
        "  plt.ylabel('True Positive Rate')\n",
        "  #plt.title('Receiver operating characteristic')\n",
        "  plt.legend(loc=\"lower right\")\n",
        "  plt.show()\n",
        "\n",
        "# plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
        "#          label='Chance', alpha=.8)\n",
        "\n",
        "  plt.plot(pmean_fpr, pmean_tpr, color=colors['Promise test set'], linestyle=':',\n",
        "           label=r'Mean p-fold (AUC = %0.2f $\\pm$ %0.2f)' % (pmean_auc, std_auc),\n",
        "           lw=2, alpha=.8)\n",
        "\n",
        "  mean_tpr = np.mean(tprs, axis=0)\n",
        "  mean_tpr[-1] = 1.0\n",
        "#   mean_auc = auc(mean_fpr, mean_tpr)\n",
        "  mean_auc = np.mean(aucs, axis=0)\n",
        "  std_auc = np.std(aucs)\n",
        "  plt.plot(mean_fpr, mean_tpr, color=colors['Promise test set'], linestyle='--',\n",
        "           label=r'Mean k-fold (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
        "           lw=2, alpha=.8)\n",
        "\n",
        "  std_tpr = np.std(tprs, axis=0)\n",
        "  tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
        "  tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
        "  plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
        "                   label=r'$\\pm$ 1 std. dev. from k-fold')\n",
        "\n",
        "\n",
        "  idx = 0\n",
        "  #colors = ['green', 'brown', 'darkolivegreen', 'purple', 'yellow', 'black', 'red', 'peru']\n",
        "  for gold in golds:\n",
        "    fpr, tpr, thresholds = roc_curve(gold, probs[idx])\n",
        "#     the_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, lw=2, color=colors[names[idx]], alpha=0.8,\n",
        "               label='%s (AUC = %0.2f)' % (names[idx], scores[idx]))\n",
        "    idx += 1\n",
        "\n",
        "\n",
        "  plt.xlim([-0.01, 1.01])\n",
        "  plt.ylim([-0.01, 1.01])\n",
        "  plt.xlabel('False Positive Rate')\n",
        "  plt.ylabel('True Positive Rate')\n",
        "  #plt.title('Receiver operating characteristic')\n",
        "  #plt.legend(loc=\"lower right\")\n",
        "\n",
        "  handles, labels = plt.gca().get_legend_handles_labels()\n",
        "  order = [2, 1, 0]\n",
        "  \n",
        "  for i in range(3, len(handles)):\n",
        "    order.append(i)\n",
        "  \n",
        "  print ('The order is', order)\n",
        "  plt.legend([handles[idx] for idx in order],[labels[idx] for idx in order],loc=\"lower right\")\n",
        "\n",
        "  #plt.show()\n",
        "  print ('roc-' + str(nrfeat) + '-' + appendix + '.pdf')\n",
        "  plt.savefig('roc-' + str(nrfeat) + '-' + appendix + '.pdf', dpi=300, bbox_inches='tight')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FgW5Yr4QRmJ5"
      },
      "source": [
        "## 3a. Precision, recall, F1, ROC curve\n",
        "\n",
        "Imports the classified and **enriched** dataset, calculates precision, recall, F1 score and plots the ROC curve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdQaEadqu-2N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "folder_datasets = './' #can be an url\n",
        "filenames = ['esa-eucl-est', 'ds2', 'ds3', 'dronology', 'reqview', 'leeds', 'wasp']\n",
        "labels = ['ESA Euclid', 'Helpdesk', 'User mgmt', 'Dronology', 'ReqView', 'Leeds library', 'WASP']\n",
        "remove = [('dronology', 'f'),('dronology', 'oq'),('wasp', 'f'),('wasp', 'oq')]\n",
        "oversample = [('ds3', 'f'), ('ds3', 'oq')]\n",
        "targets = ['IsFunctional', 'IsQuality', 'OnlyFunctional', 'OnlyQuality']\n",
        "nrfeat = 100\n",
        "\n",
        "colorpalette = ['#000000', '#e69f00', '#56b4e9', '#009e73', '#f0e442', '#0072b2', '#d55e00', '#cc79a7']\n",
        "  \n",
        "colors = {'Promise test set' : colorpalette[0]}\n",
        "for i in range(0, len(filenames)):\n",
        "  colors.update({labels[i] : colorpalette[i+1]})\n",
        "\n",
        "print (colors)\n",
        "pd.set_option('precision', 3)\n",
        "\n",
        "allfiles = ['Promise train', 'Promise test', 'Macro-average', 'Micro-average']\n",
        "allfiles += labels\n",
        "allresults = pd.DataFrame(allfiles, columns = ['Dataset']) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_xXXqVdxtfyj",
        "colab": {}
      },
      "source": [
        "for target in targets:\n",
        "  print ('Target:', target)\n",
        "  \n",
        "  if target == 'IsFunctional':\n",
        "    appendix = 'f'\n",
        "    to_drop = ['IsFunctional', 'IsQuality']\n",
        "  elif target == 'IsQuality':\n",
        "    appendix = 'q'\n",
        "    to_drop = ['IsFunctional', 'IsQuality']\n",
        "  elif target == 'OnlyQuality':\n",
        "    appendix = 'oq'\n",
        "    to_drop = ['IsFunctional', 'OnlyQuality']\n",
        "  elif target == 'OnlyFunctional':\n",
        "    appendix = 'of'\n",
        "    to_drop = ['OnlyFunctional', 'IsQuality']\n",
        "        \n",
        "  #read the promise dataset, it is used to train the classifier, which will be then tested on all other datasets\n",
        "  data = pd.read_csv(folder_datasets+'promise-km-' + str(nrfeat) + '-' + appendix + '.csv', engine='python')\n",
        "      \n",
        "  probs = []\n",
        "  names = []\n",
        "  golds = []\n",
        "  auc_scores = []\n",
        "  data = drop_descriptive_columns(data)\n",
        "\n",
        "  # split promise in 75/25\n",
        "  train_x, test_x, train_y, test_y = split_tr_te(data, target, to_drop)\n",
        "  res = []\n",
        "  #train the classifier on the 75% of promise\n",
        "  model = SVC(kernel='linear', C=1, random_state=0, probability=True)  \n",
        "  scores_line, _, _ = train_classifier(model, train_x, train_y, 'Promise train')\n",
        "  #test the performances on the remaining 25\n",
        "  scores_line, svm_te, svm_pr = evaluate_classifier(model, test_x, test_y, 'Promise test')\n",
        "  print (scores_line)\n",
        "  res.append(scores_line)\n",
        "  probs.append(svm_pr)\n",
        "  names.append('Promise test set')\n",
        "  golds.append(test_y)\n",
        "  auc_scores.append(scores_line[4])\n",
        "  \n",
        "  #retrain the classifier on entire promise and test it on the other datasets\n",
        "  model.fit(data.drop(to_drop, axis=1), data[target])\n",
        "  pred_train = model.predict(data.drop(to_drop, axis=1))\n",
        "  prob = model.predict_proba(data.drop(to_drop, axis=1))[:, 1]\n",
        "  scores_line = print_scores(data[target], pred_train, 'Promise train', prob)\n",
        "  print (scores_line)\n",
        "  res.append(scores_line)\n",
        "  \n",
        "  precisions = []\n",
        "  recalls = []\n",
        "  f1s = []\n",
        "  aucs = []\n",
        "  idx = 0\n",
        "  for filename in filenames: #loop for all datasets\n",
        "    print(filename)\n",
        "    data3 = pd.read_csv(folder_datasets+filename + '-tagged-' + str(nrfeat) + '-' + appendix + '.csv', engine='python')\n",
        "    \n",
        "    data3 = drop_descriptive_columns(data3)\n",
        "    if (filename, appendix) in oversample:\n",
        "      print ('Oversampling', filename)\n",
        "      X, y = makeOverSamplesADASYN(data3.drop(to_drop, axis=1), data3[target])\n",
        "    else:\n",
        "      X = data3.drop(to_drop, axis=1)\n",
        "      y = data3[target]\n",
        "    scores_line, svm_te, svm_pr = evaluate_classifier(model, X, y, filename)\n",
        "    precisions.append(scores_line[1])\n",
        "    recalls.append(scores_line[2])\n",
        "    f1s.append(scores_line[3])\n",
        "    aucs.append(scores_line[4])\n",
        "    res.append(scores_line)\n",
        "    if (filename, appendix) not in remove:\n",
        "      probs.append(svm_pr)\n",
        "      names.append(labels[idx])\n",
        "      auc_scores.append(scores_line[4])\n",
        "      if (filename, appendix) in oversample:\n",
        "        golds.append(y)\n",
        "      else:\n",
        "        golds.append(y.values.tolist())\n",
        "    idx = idx + 1\n",
        "  \n",
        "  res.append(['Macro-average', np.mean(precisions), np.mean(recalls), np.mean(f1s), np.mean(aucs)])\n",
        "  res.append(['Std-dev', np.std(precisions), np.std(recalls), np.std(f1s), np.std(aucs)])\n",
        "  \n",
        "  #display the results in the form of tables precision recall f1 auc, plots\n",
        "  build_plot(y_true=golds, scores=probs, labels=names)\n",
        "  make_roc_curve(appendix, target, to_drop, golds, probs, names, auc_scores, nrfeat, colors)  \n",
        "  results = pd.DataFrame(res, columns = ['Dataset', 'Prec-' + appendix, 'Rec-' + appendix, 'F1-' + appendix, 'AUC-' + appendix]) \n",
        "  display(HTML(results.to_html()))\n",
        "  \n",
        "  allresults = pd.merge(allresults, results, on='Dataset')\n",
        "  \n",
        "display(HTML(allresults.to_html()))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}